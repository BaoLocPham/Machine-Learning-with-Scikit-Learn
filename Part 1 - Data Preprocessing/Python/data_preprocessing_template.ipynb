{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_preprocessing_template.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WOw8yMd1VlnD"},"source":["# Data Preprocessing Template"]},{"cell_type":"markdown","metadata":{"id":"NvUGC8QQV6bV"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"wfFEXZC0WS-V"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MH_p-5bh5LFK","executionInfo":{"status":"ok","timestamp":1630574781420,"user_tz":-420,"elapsed":17533,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"166dce69-d169-452a-8be0-1bea750440d4"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# %cd ./drive/MyDrive/MachineLearning/Part1_Data_Preprocessing/Section_2_Part1/Python/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"fhYaZ-ENV_c5"},"source":["## Importing the dataset"]},{"cell_type":"code","metadata":{"id":"aqHTg9bxWT_u"},"source":["dataset = pd.read_csv('Data.csv')\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"4bjWhX8FLK4L","executionInfo":{"status":"ok","timestamp":1630548302593,"user_tz":-420,"elapsed":481,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"e91c0e0c-cb1f-4030-bbb3-509623b3b38c"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Country</th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Purchased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>France</td>\n","      <td>44.0</td>\n","      <td>72000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Spain</td>\n","      <td>27.0</td>\n","      <td>48000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Germany</td>\n","      <td>30.0</td>\n","      <td>54000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Spain</td>\n","      <td>38.0</td>\n","      <td>61000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Germany</td>\n","      <td>40.0</td>\n","      <td>NaN</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>France</td>\n","      <td>35.0</td>\n","      <td>58000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Spain</td>\n","      <td>NaN</td>\n","      <td>52000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>France</td>\n","      <td>48.0</td>\n","      <td>79000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Germany</td>\n","      <td>50.0</td>\n","      <td>83000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>France</td>\n","      <td>37.0</td>\n","      <td>67000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Country   Age   Salary Purchased\n","0   France  44.0  72000.0        No\n","1    Spain  27.0  48000.0       Yes\n","2  Germany  30.0  54000.0        No\n","3    Spain  38.0  61000.0        No\n","4  Germany  40.0      NaN       Yes\n","5   France  35.0  58000.0       Yes\n","6    Spain   NaN  52000.0        No\n","7   France  48.0  79000.0       Yes\n","8  Germany  50.0  83000.0        No\n","9   France  37.0  67000.0       Yes"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32F0oRqHP3KM","executionInfo":{"status":"ok","timestamp":1630548231138,"user_tz":-420,"elapsed":359,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"9588b585-1e7b-4027-b301-30a9e3d23d92"},"source":["dataset.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 4)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZ6JfVFnMDlI","executionInfo":{"status":"ok","timestamp":1630547655709,"user_tz":-420,"elapsed":4,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"21699260-1d17-4c6c-f1e5-a3afff72e7da"},"source":["X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([['France', 44.0, 72000.0],\n","       ['Spain', 27.0, 48000.0],\n","       ['Germany', 30.0, 54000.0],\n","       ['Spain', 38.0, 61000.0],\n","       ['Germany', 40.0, nan],\n","       ['France', 35.0, 58000.0],\n","       ['Spain', nan, 52000.0],\n","       ['France', 48.0, 79000.0],\n","       ['Germany', 50.0, 83000.0],\n","       ['France', 37.0, 67000.0]], dtype=object)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIo3GbMaMFyu","executionInfo":{"status":"ok","timestamp":1630547657042,"user_tz":-420,"elapsed":3,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"17997773-1504-4320-8c72-fa233f2562f5"},"source":["y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n","      dtype=object)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"up84dFgIMXuB"},"source":["## Taking care of missing data\n","\n","In this notebook, we'll use the [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n","\n","The `fit` part:\n","* Use to extract some info on the which object is applied.\n","\n","The `transform` pard:\n","* Use to apply transformation. (replace `NaN` values)."]},{"cell_type":"code","metadata":{"id":"Rinw8vO5Ma3C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630549895569,"user_tz":-420,"elapsed":351,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"31d11413-4d18-45c2-f8e6-9d0af85064dd"},"source":["import numpy as np\n","from sklearn.impute import SimpleImputer\n","## Class for dealing with missing data\n","\n","imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n","# fit the imputer\n","# Array splicing uppberbound is excluded :v\n","imputer.fit(X[:,1:3])\n","# Transform return new matrix with nan values replaced with mean values\n","X[:,1:3] = imputer.transform(X[:,1:3])\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([['France', 44.0, 72000.0],\n","       ['Spain', 27.0, 48000.0],\n","       ['Germany', 30.0, 54000.0],\n","       ['Spain', 38.0, 61000.0],\n","       ['Germany', 40.0, 63777.77777777778],\n","       ['France', 35.0, 58000.0],\n","       ['Spain', 38.77777777777778, 52000.0],\n","       ['France', 48.0, 79000.0],\n","       ['Germany', 50.0, 83000.0],\n","       ['France', 37.0, 67000.0]], dtype=object)"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"4vKPJLQjTQ9d"},"source":["## Encoding Categorical data"]},{"cell_type":"markdown","metadata":{"id":"M_NzPLh-TT5M"},"source":["### Encoding the Independent Variable\n","\n","Because the machine learning models cannot understand string and characters. We'll need a way to transform these string into numbers.\n","\n","We'll use [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bs-NzxX3TY1E","executionInfo":{"status":"ok","timestamp":1630549909017,"user_tz":-420,"elapsed":327,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"84062b3d-65f6-4ed5-d0d8-85e657a0f592"},"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# ColumnTransformer transformer argument takes an array of tuples [(name, encoder, columns)]\n","# remainder define what do you want to do with the data that transformer not apply on\n","ct = ColumnTransformer(transformers=[(\"onehotencoder\", OneHotEncoder(), [0])], remainder=\"passthrough\")\n","\n","X = np.array(ct.fit_transform(X))\n","\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n","       [0.0, 0.0, 1.0, 27.0, 48000.0],\n","       [0.0, 1.0, 0.0, 30.0, 54000.0],\n","       [0.0, 0.0, 1.0, 38.0, 61000.0],\n","       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n","       [1.0, 0.0, 0.0, 35.0, 58000.0],\n","       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n","       [1.0, 0.0, 0.0, 48.0, 79000.0],\n","       [0.0, 1.0, 0.0, 50.0, 83000.0],\n","       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"BGHnDn3KWlj-"},"source":["### Encoding the Dependent Variable\n","\n","Because the label (aka `y`) is still in string (`\"yes\"` or `\"no\"`), we need to transform it.\n","\n","We'll use [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLadr4qHWp70","executionInfo":{"status":"ok","timestamp":1630550247967,"user_tz":-420,"elapsed":6,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"c06d241a-41de-4490-9d5c-19da13c8173b"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"3abSxRqvWEIB"},"source":["## Splitting the dataset into the Training set and Test set\n","\n","In order to split the dataset into the training set and test set, there are build in function in scikit.learn called [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"]},{"cell_type":"code","metadata":{"id":"hm48sif-WWsh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630552822992,"user_tz":-420,"elapsed":332,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"7961c93e-82a8-4ebc-b15d-864d0b46cd1f"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((8, 5), (2, 5), (8,), (2,))"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qmes775gb4Jh","executionInfo":{"status":"ok","timestamp":1630552824335,"user_tz":-420,"elapsed":3,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"a1062a28-b021-4759-84c0-06968dd3b8b7"},"source":["X_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n","       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n","       [1.0, 0.0, 0.0, 44.0, 72000.0],\n","       [0.0, 0.0, 1.0, 38.0, 61000.0],\n","       [0.0, 0.0, 1.0, 27.0, 48000.0],\n","       [1.0, 0.0, 0.0, 48.0, 79000.0],\n","       [0.0, 1.0, 0.0, 50.0, 83000.0],\n","       [1.0, 0.0, 0.0, 35.0, 58000.0]], dtype=object)"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FE_lCTR_gl0Q","executionInfo":{"status":"ok","timestamp":1630552825741,"user_tz":-420,"elapsed":4,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"3d6e6785-0d13-4a51-8a22-f5288988e75a"},"source":["X_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.0, 1.0, 0.0, 30.0, 54000.0],\n","       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvTovXEMcJzg","executionInfo":{"status":"ok","timestamp":1630552827111,"user_tz":-420,"elapsed":4,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"59a9089c-598d-4714-bb2b-7b563e90a1c4"},"source":["y_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 0, 0, 1, 1, 0, 1])"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"logoSgoUIuHx"},"source":["## Feature Scaling \n","\n","Why don't we apply `Feature scaling` before the splitting of training set and test set?\n","\n","Well, because the test set must be an new set, the set that you are not working on, but the `Feature Scaling` approach is get the mean and standard deviation of the whole dataset. So if you apply the `Feature Scaling` before splitting, it will get the mean and standard deviation including the test set (the set that you aren't suppose to working with), this will cause **data leakage**. (Grab something in the test set into training set)"]},{"cell_type":"markdown","metadata":{"id":"raQrdqNfME9w"},"source":["> When should we use Standardization and Normalization:\n","  * Generally you should normalize (normalization) when the data is normally distributed, and scale (standardization) when the data is not normally distributed. In doubt, you should go for standardization. However\n","what is commonly done is that the two scaling methods are tested."]},{"cell_type":"markdown","metadata":{"id":"x7iCfNjYfO9U"},"source":["In this case, we'll use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) "]},{"cell_type":"code","metadata":{"id":"1_JDuJx1fATL"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","std_scaler = StandardScaler()\n","\n","X_train[:,-2:] = std_scaler.fit_transform(X_train[:,-2:])\n","# Because we already fit in the X_train so we only do transform in X_test\n","X_test[:,-2:] = std_scaler.transform(X_test[:,-2:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0qQUJj8ffcd","executionInfo":{"status":"ok","timestamp":1630552831096,"user_tz":-420,"elapsed":4,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"db9e04d1-a06a-4f0a-fa34-a85bb9fef4b3"},"source":["X_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.0, 0.0, 1.0, -0.19159184384578545, -1.0781259408412425],\n","       [0.0, 1.0, 0.0, -0.014117293757057777, -0.07013167641635372],\n","       [1.0, 0.0, 0.0, 0.566708506533324, 0.633562432710455],\n","       [0.0, 0.0, 1.0, -0.30453019390224867, -0.30786617274297867],\n","       [0.0, 0.0, 1.0, -1.9018011447007988, -1.420463615551582],\n","       [1.0, 0.0, 0.0, 1.1475343068237058, 1.232653363453549],\n","       [0.0, 1.0, 0.0, 1.4379472069688968, 1.5749910381638885],\n","       [1.0, 0.0, 0.0, -0.7401495441200351, -0.5646194287757332]],\n","      dtype=object)"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oK_0pfT9ge_K","executionInfo":{"status":"ok","timestamp":1630552832137,"user_tz":-420,"elapsed":3,"user":{"displayName":"Lộc Bảo","photoUrl":"","userId":"01406170824357259025"}},"outputId":"4b30c023-635a-4328-fdf4-ecac3ce95439"},"source":["X_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.0, 1.0, 0.0, -1.4661817944830124, -0.9069571034860727],\n","       [1.0, 0.0, 0.0, -0.44973664397484414, 0.2056403393225306]],\n","      dtype=object)"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"RDv1oDCpgfz3"},"source":[""],"execution_count":null,"outputs":[]}]}